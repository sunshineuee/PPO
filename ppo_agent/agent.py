from pathlib import Path

import time
from datetime import datetime

from typing import Dict

from ppo_agent.enums import SUPPORTED_TIMEFRAMES, INTERVAL_TO_TIMESPAN

import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env

from ppo_agent.data_loader import load_recent_candles
from ppo_agent.indicators import compute_all_indicators
from ppo_agent.enums import SUPPORTED_TIMEFRAMES
from ppo_agent.features import extract_features

from ppo_agent.utils import TrainingJournal
import json
import pandas as pd
from typing import Dict, Any

class DataLoader:
    def __init__(self, registry_path: str = "ASSET_REGISTRY.json"):
        self.registry_path = registry_path
        self.registry = self._load_registry()

    def _load_registry(self) -> Dict[str, Any]:
        try:
            with open(self.registry_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {"assets": {}}

    def _save_registry(self) -> None:
        with open(self.registry_path, 'w') as f:
            json.dump(self.registry, f, indent=4)

    def _register_new_figi(self, figi: str, broker: str = "TINKOFF") -> None:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ FIGI –≤ —Ä–µ–µ—Å—Ç—Ä–µ"""
        if figi not in self.registry["assets"]:
            self.registry["assets"][figi] = {
                "broker": broker,
                "first_seen": pd.Timestamp.now().isoformat()
            }
            self._save_registry()
            print(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –Ω–æ–≤—ã–π FIGI: {figi}")

    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞:
        1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö FIGI
        2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        """
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ 'figi'
        unique_figis = df['figi'].unique()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ FIGI
        for figi in unique_figis:
            self._register_new_figi(figi)
            
        # –ó–¥–µ—Å—å –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        processed_df = self._add_technical_features(df)
        
        return processed_df

    def _add_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        # –ó–¥–µ—Å—å –ª–æ–≥–∏–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        # ...
        return df

class PPOAgent:
    def __init__(self, model_dir: str = "ppo_agent/models", journal_dir: str = "training_journal"):
        self.model_dir = Path(model_dir)
        self.models: Dict[str, PPO] = {}
        self.env = DummyVecEnv([lambda: make_vec_env("CartPole-v1", n_envs=1)])
        self.journal = TrainingJournal(journal_dir)
        self.load_all_models()

    def train_from_dataframe(self, df: pd.DataFrame):
        grouped = df.groupby(['figi', 'timeframe'])
        for (figi, timeframe), group in grouped:
            if not figi or timeframe not in SUPPORTED_TIMEFRAMES:
                continue

            from_time = pd.to_datetime(group["timestamp"].min())
            to_time = pd.to_datetime(group["timestamp"].max())

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∂—É—Ä–Ω–∞–ª—É
            if self.journal.was_trained(figi, timeframe, from_time, to_time):
                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è: {figi} {timeframe} —É–∂–µ –æ–±—É—á–µ–Ω –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ {from_time} - {to_time}")
                continue

            features = extract_features(group)
            if 'reward' not in group.columns:
                print(f"‚ö†Ô∏è –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ reward: {figi} {timeframe}")
                continue

            observations = features.to_numpy()
            rewards = group['reward'].to_numpy()

            if len(observations) < 10:
                print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {figi} {timeframe}")
                continue

            print(f"üß† –û–±—É—á–µ–Ω–∏–µ {figi} {timeframe} | –ø—Ä–∏–º–µ—Ä–æ–≤: {len(observations)} | reward avg: {rewards.mean():.4f}")

            model = PPO("MlpPolicy", self.env, verbose=0)
            model.learn(total_timesteps=len(observations))
            model_path = self.model_dir / f"{figi}_{timeframe}.zip"
            model.save(model_path)
            self.models[f"{figi}_{timeframe}"] = model

            self.journal.record_training(figi, timeframe, from_time, to_time)

    def self_training_loop(self, interval_sec: float = 0.5):
        while True:
            for key in list(self.models.keys()):
                try:
                    figi, timeframe = key.rsplit("_", 1)
                    interval = SUPPORTED_TIMEFRAMES.get(timeframe)
                    if not interval:
                        continue

                    to_time = datetime.utcnow()
                    from_time = to_time - INTERVAL_TO_TIMESPAN[interval] * 100

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞, –æ–±—É—á–∞–ª—Å—è –ª–∏ —É–∂–µ —ç—Ç–æ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª
                    if self.journal.was_trained(figi, timeframe, from_time, to_time):
                        print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ {figi} {timeframe}: —É–∂–µ –æ–±—É—á–µ–Ω–æ –∑–∞ {from_time} ‚Äî {to_time}")
                        continue

                    df = load_recent_candles(figi, timeframe, 100)
                    if df.empty or len(df) < 52:
                        print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–µ—á–µ–π: {figi} {timeframe}")
                        continue

                    features_df = compute_all_indicators(df)
                    if 'reward' not in features_df.columns:
                        print(f"‚ö†Ô∏è –ù–µ—Ç reward: {figi} {timeframe}")
                        continue

                    features_df = extract_features(features_df)
                    observations = features_df.to_numpy()
                    rewards = features_df["reward"].to_numpy()

                    model = self.models[key]
                    model.learn(total_timesteps=len(observations))
                    model.save(self.model_dir / f"{figi}_{timeframe}.zip")

                    self.journal.record_training(figi, timeframe, from_time, to_time)
                    print(f"‚úÖ –î–æ–æ–±—É—á–µ–Ω–æ: {figi} {timeframe} | {len(observations)} –ø—Ä–∏–º–µ—Ä–æ–≤")

                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≤ self_training {key}: {e}")

                time.sleep(interval_sec)